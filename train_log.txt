INFO     | 2024-05-26 20:06:58 | autotrain.cli.run_llm:run:344 - Running LLM
WARNING  | 2024-05-26 20:06:59 | autotrain.trainers.common:__init__:180 - Parameters supplied but not used: deploy, config, inference, version, func, train, backend
INFO     | 2024-05-26 20:06:59 | autotrain.backends.local:create:8 - Starting local training...
INFO     | 2024-05-26 20:06:59 | autotrain.commands:launch_command:372 - ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'Mental-Health-Chatbot/training_params.json']
INFO     | 2024-05-26 20:06:59 | autotrain.commands:launch_command:373 - {'model': 'meta-llama/Llama-2-7b-chat-hf', 'project_name': 'Mental-Health-Chatbot', 'data_path': 'data/', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 1024, 'model_max_length': 1024, 'padding': None, 'trainer': 'default', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 4, 'batch_size': 4, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.045, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'text', 'rejected_text_column': 'rejected', 'push_to_hub': False, 'username': None, 'token': None}
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
INFO     | 2024-05-26 20:07:09 | autotrain.trainers.clm.train_clm_default:train:35 - Starting default/generic CLM training...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 700 examples [00:00, 14340.62 examples/s]
INFO     | 2024-05-26 20:07:09 | autotrain.trainers.clm.utils:process_input_data:388 - Train data: Dataset({
    features: ['text'],
    num_rows: 700
})
INFO     | 2024-05-26 20:07:09 | autotrain.trainers.clm.utils:process_input_data:389 - Valid data: None
INFO     | 2024-05-26 20:07:12 | autotrain.trainers.clm.utils:configure_logging_steps:461 - configuring logging steps
INFO     | 2024-05-26 20:07:12 | autotrain.trainers.clm.utils:configure_logging_steps:474 - Logging steps: 25
INFO     | 2024-05-26 20:07:12 | autotrain.trainers.clm.utils:configure_training_args:479 - configuring training args
INFO     | 2024-05-26 20:07:12 | autotrain.trainers.clm.utils:configure_block_size:542 - Using block size 1024
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO     | 2024-05-26 20:07:12 | autotrain.trainers.clm.train_clm_default:train:59 - loading model config...
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
INFO     | 2024-05-26 20:07:13 | autotrain.trainers.clm.train_clm_default:train:67 - loading model...
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [00:25<00:25, 25.14s/it]Downloading shards: 100%|██████████| 2/2 [00:35<00:00, 16.60s/it]Downloading shards: 100%|██████████| 2/2 [00:35<00:00, 17.88s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.61s/it]
INFO     | 2024-05-26 20:08:08 | autotrain.trainers.clm.train_clm_default:train:98 - model dtype: torch.float32
INFO     | 2024-05-26 20:08:08 | autotrain.trainers.clm.train_clm_default:train:101 - preparing peft model...
Running tokenizer on train dataset:   0%|          | 0/700 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1093 > 1024). Running this sequence through the model will result in indexing errors
Running tokenizer on train dataset: 100%|██████████| 700/700 [00:01<00:00, 352.87 examples/s]Running tokenizer on train dataset: 100%|██████████| 700/700 [00:01<00:00, 351.04 examples/s]
/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Grouping texts in chunks of 1024 (num_proc=4):   0%|          | 0/700 [00:00<?, ? examples/s]Grouping texts in chunks of 1024 (num_proc=4):  25%|██▌       | 175/700 [00:00<00:01, 318.78 examples/s]Grouping texts in chunks of 1024 (num_proc=4): 100%|██████████| 700/700 [00:00<00:00, 1323.14 examples/s]Grouping texts in chunks of 1024 (num_proc=4): 100%|██████████| 700/700 [00:01<00:00, 670.45 examples/s] 
INFO     | 2024-05-26 20:08:13 | autotrain.trainers.clm.train_clm_default:train:162 - creating trainer
INFO     | 2024-05-26 20:08:22 | autotrain.trainers.common:on_train_begin:231 - Starting to train...
  0%|          | 0/188 [00:00<?, ?it/s]  1%|          | 1/188 [00:40<2:05:08, 40.15s/it]  1%|          | 2/188 [01:18<2:01:53, 39.32s/it]  2%|▏         | 3/188 [01:57<2:00:23, 39.05s/it]  2%|▏         | 4/188 [02:36<1:59:19, 38.91s/it]  3%|▎         | 5/188 [03:15<1:58:26, 38.83s/it]  3%|▎         | 6/188 [03:53<1:57:38, 38.79s/it]  4%|▎         | 7/188 [04:32<1:56:53, 38.75s/it]  4%|▍         | 8/188 [05:11<1:56:10, 38.73s/it]  5%|▍         | 9/188 [05:49<1:55:29, 38.71s/it]  5%|▌         | 10/188 [06:28<1:54:49, 38.70s/it]  6%|▌         | 11/188 [07:07<1:54:08, 38.69s/it]  6%|▋         | 12/188 [07:45<1:53:29, 38.69s/it]  7%|▋         | 13/188 [08:24<1:52:49, 38.69s/it]  7%|▋         | 14/188 [09:03<1:52:10, 38.68s/it]  8%|▊         | 15/188 [09:41<1:51:31, 38.68s/it]  9%|▊         | 16/188 [10:20<1:50:52, 38.67s/it]  9%|▉         | 17/188 [10:59<1:50:13, 38.67s/it] 10%|▉         | 18/188 [11:37<1:49:34, 38.67s/it] 10%|█         | 19/188 [12:16<1:48:55, 38.67s/it] 11%|█         | 20/188 [12:55<1:48:17, 38.67s/it] 11%|█         | 21/188 [13:33<1:47:38, 38.67s/it] 12%|█▏        | 22/188 [14:12<1:47:00, 38.68s/it] 12%|█▏        | 23/188 [14:51<1:46:21, 38.68s/it] 13%|█▎        | 24/188 [15:29<1:45:43, 38.68s/it] 13%|█▎        | 25/188 [16:08<1:45:04, 38.67s/it]INFO     | 2024-05-26 20:24:30 | autotrain.trainers.common:on_log:226 - {'loss': 1.5473, 'grad_norm': 0.2690061926841736, 'learning_rate': 0.00019289940828402368, 'epoch': 0.5235602094240838}
                                                   13%|█▎        | 25/188 [16:08<1:45:04, 38.67s/it] 14%|█▍        | 26/188 [16:47<1:44:25, 38.67s/it] 14%|█▍        | 27/188 [17:25<1:43:46, 38.68s/it] 15%|█▍        | 28/188 [18:04<1:43:08, 38.68s/it] 15%|█▌        | 29/188 [18:43<1:42:29, 38.67s/it] 16%|█▌        | 30/188 [19:21<1:41:50, 38.67s/it] 16%|█▋        | 31/188 [20:00<1:41:11, 38.67s/it] 17%|█▋        | 32/188 [20:39<1:40:46, 38.76s/it] 18%|█▊        | 33/188 [21:18<1:40:03, 38.73s/it] 18%|█▊        | 34/188 [21:56<1:39:21, 38.71s/it] 19%|█▊        | 35/188 [22:35<1:38:40, 38.70s/it] 19%|█▉        | 36/188 [23:14<1:38:01, 38.69s/it] 20%|█▉        | 37/188 [23:52<1:37:21, 38.68s/it] 20%|██        | 38/188 [24:31<1:36:41, 38.68s/it] 21%|██        | 39/188 [25:10<1:36:02, 38.68s/it] 21%|██▏       | 40/188 [25:48<1:35:23, 38.67s/it] 22%|██▏       | 41/188 [26:27<1:34:45, 38.68s/it] 22%|██▏       | 42/188 [27:06<1:34:06, 38.67s/it] 23%|██▎       | 43/188 [27:44<1:33:27, 38.67s/it] 23%|██▎       | 44/188 [28:23<1:32:48, 38.67s/it] 24%|██▍       | 45/188 [29:02<1:32:18, 38.73s/it] 24%|██▍       | 46/188 [29:41<1:31:37, 38.71s/it] 25%|██▌       | 47/188 [30:19<1:30:56, 38.70s/it] 26%|██▌       | 48/188 [30:56<1:28:45, 38.04s/it] 26%|██▌       | 49/188 [31:34<1:28:34, 38.23s/it] 27%|██▋       | 50/188 [32:13<1:28:13, 38.36s/it]{'loss': 1.5473, 'grad_norm': 0.2690061926841736, 'learning_rate': 0.00019289940828402368, 'epoch': 0.52}
INFO     | 2024-05-26 20:40:35 | autotrain.trainers.common:on_log:226 - {'loss': 0.8827, 'grad_norm': 0.19231748580932617, 'learning_rate': 0.00016331360946745562, 'epoch': 1.0471204188481675}
                                                   27%|██▋       | 50/188 [32:13<1:28:13, 38.36s/it] 27%|██▋       | 51/188 [32:52<1:27:47, 38.45s/it] 28%|██▊       | 52/188 [33:30<1:27:18, 38.52s/it] 28%|██▊       | 53/188 [34:09<1:26:45, 38.56s/it] 29%|██▊       | 54/188 [34:48<1:26:11, 38.59s/it] 29%|██▉       | 55/188 [35:26<1:25:36, 38.62s/it] 30%|██▉       | 56/188 [36:05<1:24:59, 38.64s/it] 30%|███       | 57/188 [36:44<1:24:22, 38.64s/it] 31%|███       | 58/188 [37:22<1:23:44, 38.65s/it] 31%|███▏      | 59/188 [38:01<1:23:06, 38.66s/it] 32%|███▏      | 60/188 [38:40<1:22:28, 38.66s/it] 32%|███▏      | 61/188 [39:18<1:21:50, 38.66s/it] 33%|███▎      | 62/188 [39:57<1:21:11, 38.67s/it] 34%|███▎      | 63/188 [40:36<1:20:33, 38.67s/it] 34%|███▍      | 64/188 [41:14<1:19:54, 38.67s/it] 35%|███▍      | 65/188 [41:53<1:19:16, 38.67s/it] 35%|███▌      | 66/188 [42:32<1:18:37, 38.67s/it] 36%|███▌      | 67/188 [43:10<1:17:58, 38.67s/it] 36%|███▌      | 68/188 [43:49<1:17:20, 38.67s/it] 37%|███▋      | 69/188 [44:28<1:16:41, 38.67s/it] 37%|███▋      | 70/188 [45:06<1:16:02, 38.67s/it] 38%|███▊      | 71/188 [45:45<1:15:24, 38.67s/it] 38%|███▊      | 72/188 [46:24<1:14:45, 38.67s/it] 39%|███▉      | 73/188 [47:03<1:14:07, 38.67s/it] 39%|███▉      | 74/188 [47:41<1:13:28, 38.67s/it] 40%|███▉      | 75/188 [48:20<1:12:49, 38.67s/it]{'loss': 0.8827, 'grad_norm': 0.19231748580932617, 'learning_rate': 0.00016331360946745562, 'epoch': 1.05}
INFO     | 2024-05-26 20:56:42 | autotrain.trainers.common:on_log:226 - {'loss': 0.7953, 'grad_norm': 0.1805306375026703, 'learning_rate': 0.00013372781065088758, 'epoch': 1.5706806282722514}
                                                   40%|███▉      | 75/188 [48:20<1:12:49, 38.67s/it] 40%|████      | 76/188 [48:58<1:12:10, 38.67s/it] 41%|████      | 77/188 [49:37<1:11:31, 38.67s/it] 41%|████▏     | 78/188 [50:16<1:10:53, 38.66s/it] 42%|████▏     | 79/188 [50:54<1:10:14, 38.67s/it] 43%|████▎     | 80/188 [51:33<1:09:35, 38.66s/it] 43%|████▎     | 81/188 [52:12<1:08:57, 38.67s/it] 44%|████▎     | 82/188 [52:50<1:08:18, 38.67s/it] 44%|████▍     | 83/188 [53:29<1:07:40, 38.67s/it] 45%|████▍     | 84/188 [54:08<1:07:01, 38.67s/it] 45%|████▌     | 85/188 [54:46<1:06:22, 38.66s/it] 46%|████▌     | 86/188 [55:25<1:05:43, 38.66s/it] 46%|████▋     | 87/188 [56:04<1:05:05, 38.66s/it] 47%|████▋     | 88/188 [56:42<1:04:26, 38.66s/it] 47%|████▋     | 89/188 [57:21<1:03:47, 38.66s/it] 48%|████▊     | 90/188 [58:00<1:03:10, 38.67s/it] 48%|████▊     | 91/188 [58:39<1:02:31, 38.67s/it] 49%|████▉     | 92/188 [59:17<1:01:52, 38.67s/it] 49%|████▉     | 93/188 [59:56<1:01:13, 38.67s/it] 50%|█████     | 94/188 [1:00:35<1:00:35, 38.67s/it] 51%|█████     | 95/188 [1:01:13<59:56, 38.67s/it]   51%|█████     | 96/188 [1:01:50<58:17, 38.02s/it] 52%|█████▏    | 97/188 [1:02:28<57:57, 38.21s/it] 52%|█████▏    | 98/188 [1:03:07<57:31, 38.35s/it] 53%|█████▎    | 99/188 [1:03:46<57:01, 38.45s/it] 53%|█████▎    | 100/188 [1:04:24<56:29, 38.51s/it]{'loss': 0.7953, 'grad_norm': 0.1805306375026703, 'learning_rate': 0.00013372781065088758, 'epoch': 1.57}
INFO     | 2024-05-26 21:12:46 | autotrain.trainers.common:on_log:226 - {'loss': 0.7695, 'grad_norm': 0.2182900309562683, 'learning_rate': 0.00010414201183431953, 'epoch': 2.094240837696335}
                                                    53%|█████▎    | 100/188 [1:04:24<56:29, 38.51s/it] 54%|█████▎    | 101/188 [1:05:03<55:54, 38.56s/it] 54%|█████▍    | 102/188 [1:05:42<55:18, 38.59s/it] 55%|█████▍    | 103/188 [1:06:20<54:42, 38.62s/it] 55%|█████▌    | 104/188 [1:06:59<54:04, 38.63s/it] 56%|█████▌    | 105/188 [1:07:38<53:27, 38.64s/it] 56%|█████▋    | 106/188 [1:08:16<52:49, 38.65s/it] 57%|█████▋    | 107/188 [1:08:55<52:10, 38.65s/it] 57%|█████▋    | 108/188 [1:09:34<51:32, 38.66s/it] 58%|█████▊    | 109/188 [1:10:12<50:53, 38.66s/it] 59%|█████▊    | 110/188 [1:10:51<50:15, 38.66s/it] 59%|█████▉    | 111/188 [1:11:30<49:36, 38.66s/it] 60%|█████▉    | 112/188 [1:12:08<48:58, 38.66s/it] 60%|██████    | 113/188 [1:12:47<48:19, 38.66s/it] 61%|██████    | 114/188 [1:13:26<47:41, 38.66s/it] 61%|██████    | 115/188 [1:14:04<47:02, 38.66s/it] 62%|██████▏   | 116/188 [1:14:43<46:23, 38.66s/it] 62%|██████▏   | 117/188 [1:15:22<45:44, 38.66s/it] 63%|██████▎   | 118/188 [1:16:00<45:06, 38.66s/it] 63%|██████▎   | 119/188 [1:16:39<44:27, 38.66s/it] 64%|██████▍   | 120/188 [1:17:18<43:49, 38.66s/it] 64%|██████▍   | 121/188 [1:17:56<43:10, 38.66s/it] 65%|██████▍   | 122/188 [1:18:35<42:31, 38.67s/it] 65%|██████▌   | 123/188 [1:19:14<41:53, 38.67s/it] 66%|██████▌   | 124/188 [1:19:52<41:14, 38.67s/it] 66%|██████▋   | 125/188 [1:20:31<40:36, 38.67s/it]{'loss': 0.7695, 'grad_norm': 0.2182900309562683, 'learning_rate': 0.00010414201183431953, 'epoch': 2.09}
INFO     | 2024-05-26 21:28:53 | autotrain.trainers.common:on_log:226 - {'loss': 0.7225, 'grad_norm': 0.23090483248233795, 'learning_rate': 7.455621301775149e-05, 'epoch': 2.6178010471204187}
                                                    66%|██████▋   | 125/188 [1:20:31<40:36, 38.67s/it] 67%|██████▋   | 126/188 [1:21:10<39:57, 38.67s/it] 68%|██████▊   | 127/188 [1:21:48<39:18, 38.66s/it] 68%|██████▊   | 128/188 [1:22:27<38:39, 38.66s/it] 69%|██████▊   | 129/188 [1:23:06<38:01, 38.66s/it] 69%|██████▉   | 130/188 [1:23:44<37:22, 38.66s/it] 70%|██████▉   | 131/188 [1:24:23<36:43, 38.66s/it] 70%|███████   | 132/188 [1:25:02<36:05, 38.66s/it] 71%|███████   | 133/188 [1:25:40<35:26, 38.66s/it] 71%|███████▏  | 134/188 [1:26:19<34:47, 38.66s/it] 72%|███████▏  | 135/188 [1:26:58<34:09, 38.66s/it] 72%|███████▏  | 136/188 [1:27:36<33:30, 38.66s/it] 73%|███████▎  | 137/188 [1:28:15<32:51, 38.66s/it] 73%|███████▎  | 138/188 [1:28:54<32:13, 38.66s/it] 74%|███████▍  | 139/188 [1:29:32<31:34, 38.67s/it] 74%|███████▍  | 140/188 [1:30:11<30:55, 38.67s/it] 75%|███████▌  | 141/188 [1:30:50<30:17, 38.67s/it] 76%|███████▌  | 142/188 [1:31:28<29:38, 38.67s/it] 76%|███████▌  | 143/188 [1:32:07<28:59, 38.67s/it] 77%|███████▋  | 144/188 [1:32:43<27:52, 38.01s/it] 77%|███████▋  | 145/188 [1:33:22<27:23, 38.21s/it] 78%|███████▊  | 146/188 [1:34:01<26:50, 38.35s/it] 78%|███████▊  | 147/188 [1:34:39<26:16, 38.44s/it] 79%|███████▊  | 148/188 [1:35:18<25:40, 38.51s/it] 79%|███████▉  | 149/188 [1:35:57<25:03, 38.56s/it] 80%|███████▉  | 150/188 [1:36:35<24:26, 38.59s/it]{'loss': 0.7225, 'grad_norm': 0.23090483248233795, 'learning_rate': 7.455621301775149e-05, 'epoch': 2.62}
INFO     | 2024-05-26 21:44:57 | autotrain.trainers.common:on_log:226 - {'loss': 0.7022, 'grad_norm': 0.2509082555770874, 'learning_rate': 4.4970414201183434e-05, 'epoch': 3.141361256544503}
                                                    80%|███████▉  | 150/188 [1:36:35<24:26, 38.59s/it] 80%|████████  | 151/188 [1:37:14<23:48, 38.62s/it] 81%|████████  | 152/188 [1:37:53<23:10, 38.63s/it] 81%|████████▏ | 153/188 [1:38:31<22:32, 38.64s/it] 82%|████████▏ | 154/188 [1:39:10<21:54, 38.65s/it] 82%|████████▏ | 155/188 [1:39:49<21:15, 38.66s/it] 83%|████████▎ | 156/188 [1:40:27<20:37, 38.66s/it] 84%|████████▎ | 157/188 [1:41:06<19:58, 38.66s/it] 84%|████████▍ | 158/188 [1:41:45<19:19, 38.67s/it] 85%|████████▍ | 159/188 [1:42:23<18:41, 38.67s/it] 85%|████████▌ | 160/188 [1:43:02<18:02, 38.67s/it] 86%|████████▌ | 161/188 [1:43:41<17:24, 38.67s/it] 86%|████████▌ | 162/188 [1:44:19<16:45, 38.67s/it] 87%|████████▋ | 163/188 [1:44:58<16:06, 38.67s/it] 87%|████████▋ | 164/188 [1:45:37<15:28, 38.67s/it] 88%|████████▊ | 165/188 [1:46:15<14:49, 38.67s/it] 88%|████████▊ | 166/188 [1:46:54<14:10, 38.67s/it] 89%|████████▉ | 167/188 [1:47:33<13:31, 38.67s/it] 89%|████████▉ | 168/188 [1:48:11<12:53, 38.66s/it] 90%|████████▉ | 169/188 [1:48:50<12:14, 38.66s/it] 90%|█████████ | 170/188 [1:49:29<11:35, 38.66s/it] 91%|█████████ | 171/188 [1:50:07<10:57, 38.66s/it] 91%|█████████▏| 172/188 [1:50:46<10:18, 38.66s/it] 92%|█████████▏| 173/188 [1:51:25<09:39, 38.66s/it] 93%|█████████▎| 174/188 [1:52:03<09:01, 38.66s/it] 93%|█████████▎| 175/188 [1:52:42<08:22, 38.66s/it]{'loss': 0.7022, 'grad_norm': 0.2509082555770874, 'learning_rate': 4.4970414201183434e-05, 'epoch': 3.14}
INFO     | 2024-05-26 22:01:04 | autotrain.trainers.common:on_log:226 - {'loss': 0.6757, 'grad_norm': 0.28365081548690796, 'learning_rate': 1.5384615384615387e-05, 'epoch': 3.6649214659685865}
                                                    93%|█████████▎| 175/188 [1:52:42<08:22, 38.66s/it] 94%|█████████▎| 176/188 [1:53:21<07:43, 38.67s/it] 94%|█████████▍| 177/188 [1:53:59<07:05, 38.66s/it] 95%|█████████▍| 178/188 [1:54:38<06:26, 38.67s/it] 95%|█████████▌| 179/188 [1:55:17<05:47, 38.66s/it] 96%|█████████▌| 180/188 [1:55:55<05:09, 38.66s/it] 96%|█████████▋| 181/188 [1:56:34<04:30, 38.66s/it] 97%|█████████▋| 182/188 [1:57:13<03:51, 38.66s/it] 97%|█████████▋| 183/188 [1:57:51<03:13, 38.66s/it] 98%|█████████▊| 184/188 [1:58:30<02:34, 38.66s/it] 98%|█████████▊| 185/188 [1:59:09<01:55, 38.66s/it] 99%|█████████▉| 186/188 [1:59:47<01:17, 38.66s/it] 99%|█████████▉| 187/188 [2:00:26<00:38, 38.66s/it]100%|██████████| 188/188 [2:01:05<00:00, 38.66s/it]{'loss': 0.6757, 'grad_norm': 0.28365081548690796, 'learning_rate': 1.5384615384615387e-05, 'epoch': 3.66}
INFO     | 2024-05-26 22:09:27 | autotrain.trainers.common:on_log:226 - {'train_runtime': 7265.2249, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.026, 'train_loss': 0.8567672486000872, 'epoch': 3.93717277486911}
                                                   100%|██████████| 188/188 [2:01:05<00:00, 38.66s/it]100%|██████████| 188/188 [2:01:05<00:00, 38.64s/it]
{'train_runtime': 7265.2249, 'train_samples_per_second': 0.42, 'train_steps_per_second': 0.026, 'train_loss': 0.8567672486000872, 'epoch': 3.94}
INFO     | 2024-05-26 22:09:27 | autotrain.trainers.clm.utils:post_training_steps:281 - Finished training, saving model...
INFO     | 2024-05-26 22:09:30 | autotrain.cli.run_llm:run:350 - Job ID: 9648
